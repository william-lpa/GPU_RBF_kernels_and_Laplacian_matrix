{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (Evaluating RBF Kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used Packages for this assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import numba\n",
    "import math\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU Implementation\n",
    "\n",
    "In the lectures we have used a rbf evaluation using the CPU. We will use the code below for validating my solution and also benchmarking it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = .1\n",
    "npoints = 400\n",
    "nsources = 64\n",
    "\n",
    "plot_grid = np.mgrid[0:1:npoints * 1j, 0:1:npoints * 1j]\n",
    "\n",
    "targets_xy = np.vstack((plot_grid[0].ravel(),\n",
    "                        plot_grid[1].ravel(),\n",
    "                        np.zeros(plot_grid[0].size))).T\n",
    "targets_xz = np.vstack((plot_grid[0].ravel(),\n",
    "                        np.zeros(plot_grid[0].size),\n",
    "                        plot_grid[1].ravel())).T\n",
    "targets_yz = np.vstack((np.zeros(plot_grid[0].size),\n",
    "                        plot_grid[0].ravel(),\n",
    "                        plot_grid[1].ravel())).T\n",
    "\n",
    "targets = np.vstack((targets_xy, targets_xz, targets_yz))\n",
    "result_cpu = np.zeros(len(targets), dtype=np.float64)\n",
    "rand = np.random.RandomState(0)\n",
    "\n",
    "# We are picking random sources,\n",
    "\n",
    "sources = rand.rand(nsources, 3)\n",
    "weights = rand.rand(len(sources))\n",
    "\n",
    "@numba.njit(parallel=True)\n",
    "def rbf_evaluation_cpu(sources, targets, weights, result):\n",
    "    \"\"\"Evaluate the RBF sum.\"\"\"\n",
    "\n",
    "    n = len(sources)\n",
    "    m = len(targets)\n",
    "\n",
    "    result[:] = 0\n",
    "    for index in numba.prange(m):\n",
    "        result[index] = np.sum(\n",
    "            np.exp(-np.sum(np.abs(targets[index] - sources)**2, axis=1) / (2 * sigma**2)) * weights)\n",
    "    \n",
    "rbf_evaluation_cpu(sources, targets, weights, result_cpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Implementation\n",
    "The code block below is the same implementation as above but we are using GPU Cuda acceleration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16 targets and 32 sources\n",
    "SX = 16\n",
    "SY = 32\n",
    "\n",
    "#Launch threadblocks that evaluate the partial sum for 16 targets and 32 sources at a time\n",
    "@cuda.jit()\n",
    "def rbf_evaluation_cuda(sources, targets, weights, inter_results):\n",
    "    #local results only loads 32 sources for each block\n",
    "    local_result = cuda.shared.array((SX, SY), numba.float32)\n",
    "    local_targets = cuda.shared.array((SX, 3), numba.float32)\n",
    "    local_sources = cuda.shared.array((SY, 3), numba.float32)\n",
    "    local_weights = cuda.shared.array(SY, numba.float32)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "\n",
    "    px, py = cuda.grid(2)\n",
    "    \n",
    "    #checking boundaries to only process valid px,py\n",
    "    if px < targets.shape[0] or py < sources.shape[0]:\n",
    "\n",
    "        # At first we are loading all the targets into the shared memory\n",
    "        # We use only the first column of threads to do this.\n",
    "        if ty == 0:\n",
    "            for index in range(3):\n",
    "                local_targets[tx, index] = targets[px, index]\n",
    "\n",
    "        # We are now loading all the sources and weights.\n",
    "        # We only require the first row of threads to do this.\n",
    "        if tx == 0:\n",
    "            for index in range(3):\n",
    "                local_sources[ty, index] = sources[py, index]\n",
    "            # up to 32 weights\n",
    "            local_weights[ty] = weights[py]\n",
    "\n",
    "        # Let us now sync all threads\n",
    "\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Now compute the interactions\n",
    "\n",
    "        squared_diff = numba.float32(0)\n",
    "\n",
    "        for index in range(3):\n",
    "            squared_diff += (local_targets[tx, index] -\n",
    "                             local_sources[ty, index])**2\n",
    "        local_result[tx, ty] = math.exp(-squared_diff / (\n",
    "            numba.float32(2) * numba.float32(sigma)**2)) * local_weights[ty]\n",
    "\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "\n",
    "        # Now sum up all the local results\n",
    "        if ty == 0:\n",
    "            res = numba.float32(0)\n",
    "            for index in range(SY):\n",
    "                res += local_result[tx, index]\n",
    "            # intermediate results that is of size (m, p).    \n",
    "            inter_results[px, cuda.blockIdx.y] = res\n",
    "\n",
    "#launch a summation kernel of m threads, where each thread sums up the p numbers \n",
    "@cuda.jit()\n",
    "def summation_kernel(inter_results, results, targets):\n",
    "    px, py = cuda.grid(2)\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    #checking boundaries for safety, would not be a big deal since the last block,\n",
    "    # we would be summing zero inter. values\n",
    "    if px < targets.shape[0]:\n",
    "        res = numba.float32(0)\n",
    "        for index in range(p):\n",
    "            #intermediate array (m, p) \n",
    "            res += inter_results[px, index]\n",
    "            #store into a result array of dimension m\n",
    "            results[px] = res\n",
    "\n",
    "\n",
    "            \n",
    "#blocks per grid\n",
    "l = (targets.shape[0] + SX - 1) // SX\n",
    "p = (sources.shape[0] + SY - 1) // SY\n",
    "\n",
    "#CUDA memory transfer functions to copy the sources and targets to the compute device\n",
    "sources_global_mem = cuda.to_device(sources.astype('float32'))\n",
    "targets_global_mem = cuda.to_device(targets.astype('float32'))\n",
    "weights_global_mem = cuda.to_device(weights.astype('float32'))\n",
    "inter_result_global_mem = cuda.device_array((len(targets), p), dtype=np.float32)\n",
    "result_global_mem = cuda.device_array(len(targets), dtype=np.float32)\n",
    "\n",
    "rbf_evaluation_cuda[(l, p), (SX, SY)](\n",
    "        sources_global_mem, targets_global_mem, weights_global_mem, inter_result_global_mem)\n",
    "\n",
    "#Got the intermediate results,\n",
    "#We do not want to transfer the intermediate array back and forth between CPU and GPU\n",
    "\n",
    "#summation kernel, l * SX gets all targets and sums inter.results p\n",
    "summation_kernel[(l, 1), (SX, 1)](inter_result_global_mem, result_global_mem,targets_global_mem)\n",
    "\n",
    "#now we can copy the memory back to the host\n",
    "result_gpu = result_global_mem.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(result, npoints):\n",
    "    \"\"\"A helper function for visualization\"\"\"\n",
    "\n",
    "    result_xy = result[: npoints * npoints].reshape(npoints, npoints).T\n",
    "    result_xz = result[npoints * npoints: 2 *\n",
    "                       npoints * npoints].reshape(npoints, npoints).T\n",
    "    result_yz = result[2 * npoints * npoints:].reshape(npoints, npoints).T\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    im = ax.imshow(result_xy, extent=[0, 1, 0, 1], origin='lower')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "    ax = fig.add_subplot(1, 3, 2)\n",
    "    im = ax.imshow(result_xz, extent=[0, 1, 0, 1], origin='lower')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('z')\n",
    "\n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    im = ax.imshow(result_yz, extent=[0, 1, 0, 1], origin='lower')\n",
    "    ax.set_xlabel('y')\n",
    "    ax.set_ylabel('z')\n",
    "    \n",
    "visualize(result_gpu, npoints)\n",
    "visualize(result_cpu, npoints)\n",
    "\n",
    "#rtol=1e-05, atol=1e-08\n",
    "print(\"Do the results agree up to the defined tolerance? \",\n",
    "      np.allclose(result_gpu, result_cpu))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution was computed using 400 points and 64 sources.\n",
    "\n",
    "The cpu implementation uses double precision for the results but you can see the results still agree allowing the given tolerance due to different precision. The gpu kernel runs the computation using single precision numbers, which gives us some performance advantage due to its architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark results\n",
    "\n",
    "The code below demonstrates how long it takes to run the cpu vs gpu computation using 800 points and 256 sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_parameters(npoints, nsources):\n",
    "    plot_grid = np.mgrid[0:1:npoints * 1j, 0:1:npoints * 1j]\n",
    "\n",
    "    targets_xy = np.vstack((plot_grid[0].ravel(),\n",
    "                            plot_grid[1].ravel(),\n",
    "                            np.zeros(plot_grid[0].size))).T\n",
    "    targets_xz = np.vstack((plot_grid[0].ravel(),\n",
    "                            np.zeros(plot_grid[0].size),\n",
    "                            plot_grid[1].ravel())).T\n",
    "    targets_yz = np.vstack((np.zeros(plot_grid[0].size),\n",
    "                            plot_grid[0].ravel(),\n",
    "                            plot_grid[1].ravel())).T\n",
    "\n",
    "    targets = np.vstack((targets_xy, targets_xz, targets_yz))\n",
    "    result_cpu = np.zeros(len(targets), dtype=np.float64)\n",
    "    rand = np.random.RandomState(0)\n",
    "\n",
    "    sources = rand.rand(nsources, 3)\n",
    "    weights = rand.rand(len(sources))\n",
    "    return (targets, result_cpu, sources, weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = .1\n",
    "npoints = 800\n",
    "nsources = 256\n",
    "\n",
    "(targets, result_cpu, sources, weights)= generate_parameters(npoints, nsources)\n",
    "\n",
    "sources_global_mem = cuda.to_device(sources.astype('float32'))\n",
    "targets_global_mem = cuda.to_device(targets.astype('float32'))\n",
    "weights_global_mem = cuda.to_device(weights.astype('float32'))\n",
    "result_global_mem = cuda.device_array(len(targets), dtype=np.float32)\n",
    "inter_result_global_mem = cuda.device_array((len(targets), p), dtype=np.float32)\n",
    "l = (targets.shape[0] + SX - 1) // SX\n",
    "p = (sources.shape[0] + SY - 1) // SY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23 s ± 50.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rbf_evaluation_cpu(sources, targets, weights, result_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 20\n",
    "rbf_evaluation_cuda[(l, p), (SX, SY)](sources_global_mem, targets_global_mem, weights_global_mem, inter_result_global_mem)\n",
    "summation_kernel[(l, 1), (SX, 1)](inter_result_global_mem, result_global_mem,targets_global_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to increase the number of sources and targets of the GPU computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sigma = .1\n",
    "npoints = 500\n",
    "nsources = 512\n",
    "(targets, result_cpu, sources, weights)= generate_parameters(npoints, nsources)\n",
    "sources_global_mem = cuda.to_device(sources.astype('float32'))\n",
    "targets_global_mem = cuda.to_device(targets.astype('float32'))\n",
    "weights_global_mem = cuda.to_device(weights.astype('float32'))\n",
    "result_global_mem = cuda.device_array(len(targets), dtype=np.float32)\n",
    "l = (targets.shape[0] + SX - 1) // SX\n",
    "p = (sources.shape[0] + SY - 1) // SY\n",
    "result_gpu = result_global_mem.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit -n 20\n",
    "rbf_evaluation_cuda[(l, p), (SX, SY)](sources_global_mem, targets_global_mem, weights_global_mem, inter_result_global_mem)\n",
    "summation_kernel[(l, 1), (SX, 1)](inter_result_global_mem, result_global_mem,targets_global_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the GPU computation has no problems to process 750000x3 targets and 512x3 sources. The bottleneck seems to be the cell that allocates and transfers the memory from CPU and GPU. Basically, almost all the time took to run the algorithm was just to allocate and transfer data from CPU to the GPU. \n",
    "\n",
    "Numba is not freeing up memory previously allocated in different cells since the context is being shared. It takes much longer to run the first time or second time. We can still use though the sys time to compare with the actual computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (Evaluating the discrete Laplacian on GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lectures we have generated a sparse matrix which discretises Laplace operator with zero boundary conditions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15\n",
    "nelements = 5 * N**2 - 16 * N + 16\n",
    "def discretise_poisson_cpu(N):\n",
    "    \"\"\"Generate the matrix and rhs associated with the discrete Poisson operator.\"\"\"\n",
    "\n",
    "    nelements = 5 * N**2 - 16 * N + 16\n",
    "    row_ind = np.empty(nelements, dtype=np.float64)\n",
    "    col_ind = np.empty(nelements, dtype=np.float64)\n",
    "    data = np.zeros(nelements, dtype=np.float64)\n",
    "    \n",
    "    f = np.empty(N * N, dtype=np.float64)\n",
    "\n",
    "    count = 0\n",
    "    for j in range(N):\n",
    "        for i in range(N):\n",
    "            if i == 0 or i == N - 1 or j == 0 or j == N - 1:\n",
    "                row_ind[count] = col_ind[count] = j * N + i\n",
    "                data[count] = 1\n",
    "                f[j * N + i] = 0\n",
    "                count += 1\n",
    "\n",
    "            else:\n",
    "                row_ind[count: count + 5] = j * N + i\n",
    "                col_ind[count] = j * N + i\n",
    "                col_ind[count + 1] = j * N + i + 1\n",
    "                col_ind[count + 2] = j * N + i - 1\n",
    "                col_ind[count + 3] = (j + 1) * N + i\n",
    "                col_ind[count + 4] = (j - 1) * N + i\n",
    "\n",
    "                data[count] = 4 * (N - 1)**2\n",
    "                data[count + 1: count + 5] = - (N - 1)**2\n",
    "                f[j * N + i] = 1\n",
    "\n",
    "                count += 5\n",
    "    return coo_matrix((data, (row_ind, col_ind)), shape=(N**2, N**2)).tocsr(), f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Implementation\n",
    "Create N² threads and given a one-dimensional array of values ui,j in the unit square grid evaluates this discrete Laplace operator without explicity creatiing a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def evaluate_discrete_laplace(vec_in, vec_out, N):\n",
    "        # Evaluate the discrete Laplace operator\n",
    "        \n",
    "        i, j = cuda.grid(2)\n",
    "        \n",
    "        if i >= N:\n",
    "            return\n",
    "        if j >= N:\n",
    "            return\n",
    "        \n",
    "        # Compute the vector index\n",
    "        k = j * N + i\n",
    "        \n",
    "        if i == 0 or i == N - 1 or j == 0 or j == N - 1:\n",
    "            # We are at the boundary\n",
    "            # Here, the matrix just acts like the identity\n",
    "            vec_out[k] = vec_in[k]\n",
    "            return\n",
    "        \n",
    "        # Now deal with the interior element\n",
    "        \n",
    "        up = vec_in[(j + 1) * N + i]\n",
    "        down = vec_in[(j - 1) * N + i]\n",
    "        left = vec_in[j * N + i - 1]\n",
    "        right = vec_in[j * N + i + 1]\n",
    "        center = vec_in[k]\n",
    "        \n",
    "        vec_out[k] = (N - 1)**2 * (numba.float32(4) * center - up - down - left - right)\n",
    "        \n",
    "def eval_gpu(x, N):\n",
    "        #Evaluate the discrete Laplacian on the GPU.\n",
    "        \n",
    "        res = np.empty(N * N, dtype=np.float32)\n",
    "        \n",
    "        nblocks = (N + 31) // 32\n",
    "        evaluate_discrete_laplace[(nblocks, nblocks), (32, 32)](x.astype('float32'), res, N)\n",
    "        return res.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "Now let's validate our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative errorr: 1.1318467088783189e-07.\n"
     ]
    }
   ],
   "source": [
    "rand = np.random.RandomState(0)\n",
    "\n",
    "N = 500\n",
    "\n",
    "A, _ = discretise_poisson_cpu(N)\n",
    "x = rand.randn(N * N)\n",
    "\n",
    "y_cpu = A @ x\n",
    "y_gpu = eval_gpu(x, N)\n",
    "\n",
    "rel_error = np.linalg.norm(y_cpu - y_gpu, np.inf) / np.linalg.norm(y_cpu, np.inf)\n",
    "print(f\"Relative errorr: {rel_error}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the implementations is in the order of 32 bit machine precision, which is expected. Let us now do some benchmark comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark results\n",
    "\n",
    "The code below demonstrates how long it takes to run the cpu vs gpu computation having a grid of 2250000 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small size benchmark (n=100)\n",
      "----------------------------\n",
      "CPU Benchmark\n",
      "38.7 µs ± 164 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "GPU Benchmark\n",
      "688 µs ± 2.45 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "Large size benchmark (n=1000)\n",
      "-------------------------------\n",
      "CPU Benchmark\n",
      "4.83 ms ± 50.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "GPU Benchmark\n",
      "4.04 ms ± 32.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "def benchmark(N):\n",
    "    \"\"\"Benchmark the CPU and GPU implementation.\"\"\"\n",
    "    \n",
    "    A, _ = discretise_poisson_cpu(N)\n",
    "    x = rand.randn(N * N)\n",
    "    \n",
    "    print(\"CPU Benchmark\")\n",
    "    %timeit y_cpu = A @ x\n",
    "    \n",
    "    print(\"GPU Benchmark\")\n",
    "    %timeit y_gpu = eval_gpu(x, N)\n",
    "\n",
    "print(\"Small size benchmark (n=100)\")\n",
    "print(\"----------------------------\")\n",
    "benchmark(100)\n",
    "print(\"Large size benchmark (n=1000)\")\n",
    "print(\"-------------------------------\")\n",
    "benchmark(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, these benchmarks show that the GPU is faster for computing unknown points in a grid since we can load the memory only once and compute the five stencil points from the global memory. The radio of data will change based on the grid size, having bigger grids, we will have to move more interaction points from and to the CPU and GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How we could use shared memory to reduce global memory access\n",
    "\n",
    "The difficult point of this problem is the 5 point stencil. Since the shared memory is only accessible within the block, we could have a local boundary point in a block that would not be a boundary value globally (ty = tx = 32, in  100x100 grid). So the solution to make this algorithm even quicker by using shared memory would be:\n",
    "\n",
    "- load all grid points for that block when ty or tx are  0\n",
    "- if tx is zero but px (global id) is not, load grid point for that block plus the previous column point grid_points(px−1,py)\n",
    "- if ty is zero but py (global id) is not, load grid point for that block plus the previous row point grid_points(px,py-1)\n",
    "- if tx is the last local thread for that block but px is not N-1, load grid point for that block plus the next column point grid_points(px+1,py)\n",
    "- if ty is the last local thread for that block but py is not N-1, load grid point for that block plus the next row point grid_points(px,py +1)\n",
    "\n",
    "Then we could read all 5 stencil points from the grid using the local shared memory and compute the approximated value. Each thread that is an interior point would write the approximation to the result array, which is allocated in the global memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
